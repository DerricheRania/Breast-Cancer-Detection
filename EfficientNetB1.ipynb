{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29a4b32",
   "metadata": {},
   "source": [
    "### **ðŸ§© Importing all necessary libraries and Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Paths\n",
    "train_dir = 'train'  # adjust to your dataset folder\n",
    "test_dir = 'test'    # test folder with images\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda50ab6",
   "metadata": {},
   "source": [
    "### EfficientNetB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d32d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# ðŸ§  EfficientNetB1 - Frozen\n",
    "# ======================\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e442a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "\u001b[1m27018416/27018416\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model_b1 = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base_model_b1.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model_b1.output)\n",
    "x = Dropout(0.4)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model_b1 = Model(inputs=base_model_b1.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1a84e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 656ms/step - accuracy: 0.5070 - loss: 0.7038 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 565ms/step - accuracy: 0.5082 - loss: 0.6952 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 554ms/step - accuracy: 0.5344 - loss: 0.6949 - val_accuracy: 0.3143 - val_loss: 0.6940\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 555ms/step - accuracy: 0.4759 - loss: 0.7026 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 549ms/step - accuracy: 0.5086 - loss: 0.6951 - val_accuracy: 0.5000 - val_loss: 0.6942\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 546ms/step - accuracy: 0.5124 - loss: 0.6992 - val_accuracy: 0.5000 - val_loss: 0.6942\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 542ms/step - accuracy: 0.4773 - loss: 0.7017 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 540ms/step - accuracy: 0.5178 - loss: 0.6928 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 553ms/step - accuracy: 0.4749 - loss: 0.7044 - val_accuracy: 0.3571 - val_loss: 0.6940\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 543ms/step - accuracy: 0.5131 - loss: 0.6978 - val_accuracy: 0.4357 - val_loss: 0.6940\n"
     ]
    }
   ],
   "source": [
    "model_b1.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_b1 = model_b1.fit(train_generator, validation_data=val_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707391ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 793ms/step\n",
      "âœ… Validation Accuracy: 0.5071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "# Get validation predictions\n",
    "val_generator.reset()\n",
    "val_preds = model_b1.predict(val_generator)\n",
    "val_preds_classes = (val_preds > 0.5).astype(int)\n",
    "\n",
    "# True labels\n",
    "true_labels = val_generator.classes\n",
    "\n",
    "# Compute accuracy\n",
    "val_accuracy = accuracy_score(true_labels, val_preds_classes)\n",
    "print(f\"âœ… Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29429bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step\n",
      "âœ… Saved EfficientNetB0.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict test set\n",
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir, target_size=IMG_SIZE, batch_size=1, class_mode=None, shuffle=False)\n",
    "preds_b0 = model_b1.predict(test_gen)\n",
    "results_b0 = pd.DataFrame({\n",
    "    'image file': [os.path.basename(f) for f in test_gen.filenames],\n",
    "    'label': ['M' if p > 0.5 else 'N' for p in preds_b0]\n",
    "})\n",
    "results_b0.to_csv('EfficientNetB1.csv', index=False)\n",
    "print(\"âœ… Saved EfficientNetB0.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5909ae9",
   "metadata": {},
   "source": [
    "- The code that gave the highest score in kaggle is the one using B1 without fine tuning, i just put them in the same notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1c975",
   "metadata": {},
   "source": [
    "### EfficientNetB1 fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1858c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation\n",
    "train_datagen_aug = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "train_gen_aug = train_datagen_aug.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_gen_aug = train_datagen_aug.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "322d736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 705ms/step - accuracy: 0.5312 - loss: 0.6960 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 610ms/step - accuracy: 0.5353 - loss: 0.6948 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 617ms/step - accuracy: 0.5183 - loss: 0.6986 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 613ms/step - accuracy: 0.5002 - loss: 0.7087 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 625ms/step - accuracy: 0.5157 - loss: 0.6974 - val_accuracy: 0.4929 - val_loss: 0.6930\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 640ms/step - accuracy: 0.4603 - loss: 0.7065 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 633ms/step - accuracy: 0.4782 - loss: 0.7105 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 621ms/step - accuracy: 0.5372 - loss: 0.6871 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 653ms/step - accuracy: 0.4826 - loss: 0.7020 - val_accuracy: 0.5286 - val_loss: 0.6930\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 681ms/step - accuracy: 0.5171 - loss: 0.7124 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "base_model_b1_ft = EfficientNetB1(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base_model_b1_ft.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model_b1_ft.output)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model_b1_ft = Model(inputs=base_model_b1_ft.input, outputs=output)\n",
    "\n",
    "model_b1_ft.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_b1_ft_1 = model_b1_ft.fit(train_gen_aug, validation_data=val_gen_aug, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac9a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradual unfreezing (last 30 layers)\n",
    "base_model_b1_ft.trainable = True\n",
    "for layer in base_model_b1_ft.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_b1_ft.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0d6ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 798ms/step - accuracy: 0.5540 - loss: 0.6984 - val_accuracy: 0.5000 - val_loss: 0.6934 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 713ms/step - accuracy: 0.5487 - loss: 0.7082 - val_accuracy: 0.5000 - val_loss: 0.6940 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 714ms/step - accuracy: 0.4962 - loss: 0.7171 - val_accuracy: 0.5000 - val_loss: 0.6945 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - accuracy: 0.4560 - loss: 0.7289\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 708ms/step - accuracy: 0.4568 - loss: 0.7288 - val_accuracy: 0.5000 - val_loss: 0.6946 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 719ms/step - accuracy: 0.4583 - loss: 0.7270 - val_accuracy: 0.5000 - val_loss: 0.6947 - learning_rate: 5.0000e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 715ms/step - accuracy: 0.5167 - loss: 0.7022 - val_accuracy: 0.5000 - val_loss: 0.6946 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "early_stop_b1 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_sched_b1 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "history_b1_ft_2 = model_b1_ft.fit(train_gen_aug, validation_data=val_gen_aug, epochs=20,\n",
    "                                  callbacks=[early_stop_b1, lr_sched_b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c1d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 42ms/step\n",
      "âœ… Saved EfficientNetB1_Finetuned_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict test set\n",
    "preds_b1_ft = model_b1_ft.predict(test_gen)\n",
    "results_b1_ft = pd.DataFrame({\n",
    "    'image file': [os.path.basename(f) for f in test_gen.filenames],\n",
    "    'label': ['M' if p > 0.5 else 'N' for p in preds_b1_ft]\n",
    "})\n",
    "results_b1_ft.to_csv('EfficientNetB1_Ft.csv', index=False)\n",
    "print(\"âœ… Saved EfficientNetB1_Finetuned_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcd621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
