{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f30c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# ðŸ§© Imports and Paths\n",
    "# ======================\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Paths\n",
    "train_dir = 'train'  \n",
    "test_dir = 'test'    \n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87613a89",
   "metadata": {},
   "source": [
    "### EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ce5d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# ðŸ§  EfficientNetB0 - Frozen\n",
    "# ======================\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e135e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "base_model_b0 = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base_model_b0.trainable = False  # freeze layers\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model_b0.output)\n",
    "x = Dropout(0.4)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model_b0 = Model(inputs=base_model_b0.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ffc5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 522ms/step - accuracy: 0.5067 - loss: 0.7019 - val_accuracy: 0.5000 - val_loss: 0.6938\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 458ms/step - accuracy: 0.4973 - loss: 0.6918 - val_accuracy: 0.5000 - val_loss: 0.6929\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 431ms/step - accuracy: 0.4878 - loss: 0.7075 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 460ms/step - accuracy: 0.4978 - loss: 0.7008 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 446ms/step - accuracy: 0.5228 - loss: 0.6915 - val_accuracy: 0.5000 - val_loss: 0.6929\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 442ms/step - accuracy: 0.4806 - loss: 0.7035 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 470ms/step - accuracy: 0.4925 - loss: 0.6993 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 429ms/step - accuracy: 0.4457 - loss: 0.7079 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 479ms/step - accuracy: 0.4795 - loss: 0.7013 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 463ms/step - accuracy: 0.4945 - loss: 0.6964 - val_accuracy: 0.5000 - val_loss: 0.6930\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "model_b0.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_b0 = model_b0.fit(train_generator, validation_data=val_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb70a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 687ms/step\n",
      "âœ… Validation Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "# Get validation predictions\n",
    "val_generator.reset()\n",
    "val_preds = model_b0.predict(val_generator)\n",
    "val_preds_classes = (val_preds > 0.5).astype(int)\n",
    "\n",
    "# True labels\n",
    "true_labels = val_generator.classes\n",
    "\n",
    "# Compute accuracy\n",
    "val_accuracy = accuracy_score(true_labels, val_preds_classes)\n",
    "print(f\"âœ… Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b296e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step\n",
      "âœ… Saved EfficientNetB0_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict test set\n",
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir, target_size=IMG_SIZE, batch_size=1, class_mode=None, shuffle=False)\n",
    "preds_b0 = model_b0.predict(test_gen)\n",
    "results_b0 = pd.DataFrame({\n",
    "    'image file': [os.path.basename(f) for f in test_gen.filenames],\n",
    "    'label': ['M' if p > 0.5 else 'N' for p in preds_b0]\n",
    "})\n",
    "results_b0.to_csv('EfficientNetB0_predictions.csv', index=False)\n",
    "print(\"âœ… Saved EfficientNetB0_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87760c",
   "metadata": {},
   "source": [
    "### EfficientNetB0 fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29718cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 560 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# âš™ï¸ EfficientNetB0 Fine-Tuning + Strong Augmentation\n",
    "# ======================\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen_aug = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "train_gen_aug = train_datagen_aug.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_gen_aug = train_datagen_aug.flow_from_directory(\n",
    "    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='binary', subset='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9104fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "base_model_ft = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "base_model_ft.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model_ft.output)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model_ft = Model(inputs=base_model_ft.input, outputs=output)\n",
    "\n",
    "model_ft.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a93f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 602ms/step - accuracy: 0.5098 - loss: 0.7088 - val_accuracy: 0.5000 - val_loss: 0.6941\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 527ms/step - accuracy: 0.4794 - loss: 0.7000 - val_accuracy: 0.5000 - val_loss: 0.6934\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 539ms/step - accuracy: 0.5444 - loss: 0.6923 - val_accuracy: 0.4929 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 524ms/step - accuracy: 0.5054 - loss: 0.7036 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 535ms/step - accuracy: 0.4950 - loss: 0.7052 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 609ms/step - accuracy: 0.5086 - loss: 0.7034 - val_accuracy: 0.5000 - val_loss: 0.6933\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 827ms/step - accuracy: 0.5105 - loss: 0.7004 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 591ms/step - accuracy: 0.5289 - loss: 0.7006 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 581ms/step - accuracy: 0.4844 - loss: 0.7067 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 522ms/step - accuracy: 0.5253 - loss: 0.6981 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
      "Epoch 1/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 637ms/step - accuracy: 0.5134 - loss: 0.7072 - val_accuracy: 0.5000 - val_loss: 0.6936 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 566ms/step - accuracy: 0.4958 - loss: 0.7044 - val_accuracy: 0.5000 - val_loss: 0.6952 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 555ms/step - accuracy: 0.4386 - loss: 0.7232 - val_accuracy: 0.5000 - val_loss: 0.6976 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452ms/step - accuracy: 0.4966 - loss: 0.7062\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 559ms/step - accuracy: 0.4965 - loss: 0.7062 - val_accuracy: 0.5000 - val_loss: 0.7000 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 560ms/step - accuracy: 0.5044 - loss: 0.7080 - val_accuracy: 0.5000 - val_loss: 0.7016 - learning_rate: 5.0000e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 554ms/step - accuracy: 0.4679 - loss: 0.7122 - val_accuracy: 0.5000 - val_loss: 0.7028 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Train frozen base\n",
    "history_ft_1 = model_ft.fit(train_gen_aug, validation_data=val_gen_aug, epochs=10)\n",
    "\n",
    "# Stage 2: Gradual unfreezing (fine-tune last 20 layers)\n",
    "base_model_ft.trainable = True\n",
    "for layer in base_model_ft.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_ft.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_sched = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "history_ft_2 = model_ft.fit(train_gen_aug, validation_data=val_gen_aug, epochs=20,\n",
    "                            callbacks=[early_stop, lr_sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f4c5849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m300/300\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step\n",
      "âœ… Saved EfficientNetB0_Finetuned_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict test set\n",
    "preds_ft = model_ft.predict(test_gen)\n",
    "results_ft = pd.DataFrame({\n",
    "    'image file': [os.path.basename(f) for f in test_gen.filenames],\n",
    "    'label': ['M' if p > 0.5 else 'N' for p in preds_ft]\n",
    "})\n",
    "results_ft.to_csv('EfficientNetB0_Finetuned_predictions.csv', index=False)\n",
    "print(\"âœ… Saved EfficientNetB0_Finetuned_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51081b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
